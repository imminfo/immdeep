# RNN models used for modelling

## Contents

- One-way LSTM

- One-way LSTM with receptors with small proportions

- One-way LSTM with receptors with small proportions, weighted

- One-way LSTM with 256 hidden nodes with receptors with small proportions, weighted


## One-way LSTM

Model: LSTM on the first 30K data

File: models/model.lstm128.bn.dense.hdf5

After more than 100 iterations here are results for the first 20 receptors:
```
[[  5.0744682   11.84071827]
 [  5.22786273  11.8524065 ]
 [  5.50443683  11.84071827]
 [  6.25304755  11.89706039]
 [  6.33391     11.79355621]
 [  6.38307337  11.76680374]
 [  6.37860409  11.83020401]
 [  6.5170848   11.84064388]
 [  6.5273809   11.85927868]
 [  6.60817853  11.79740143]
 [  6.61521585  11.70992184]
 [  6.71463049  11.93083572]
 [  6.94914894  11.79218006]
 [  6.92190991  11.87408543]
 [  6.92383114  11.79092789]
 [  7.0442619   11.84865761]
 [  7.17130679  11.81984138]
 [  7.16884676  11.81451893]
 [  7.17130679  11.85754585]
 [  7.21923525  11.82061195]]
```

Despite quite low loss, the proportions are looking bad. It seems that the optimizer
"narrowed" the proportions in order to minimize loss (I checked - all proporions almost the same). 
I'm pretty sure that such narrowing is due to the very heavy tail of receptors with 
almost identical proportions (~ 11.8 in neg-log-scale).


## One-way LSTM with receptors with small proportions

Model: LSTM at each step trained on ~100 receptors with “big” proportions (count >= 4), and ~28 receptors with “small” proportions (count < 3); all of them randomly sampled from the input data.

File: models/model.lstm128.bn.dense.mix.hdf5

Changed loss to MSE, quite helpful.

After ~180 iterations:
```
Predict big proportions:
        real            pred
[[  5.0744682    2.11004472]
 [  5.50443683   2.11004472]
 [  6.25304755  10.26939869]
 [  6.33391      0.        ]
 [  6.38307337   0.        ]
 [  6.37860409  10.14752769]
 [  6.5170848   10.24857044]
 [  6.5273809   10.22775173]
 [  6.60817853  10.14358807]
 [  6.61521585  10.2518549 ]
 [  6.71463049   0.        ]
 [  6.94914894  10.16672897]
 [  6.92190991  10.25177383]
 [  6.92383114  10.26767731]
 [  7.0442619   10.24433613]
 [  7.17130679  10.18964672]
 [  7.16884676  10.24189472]
 [  7.17130679   9.93906689]
 [  7.21923525  10.27545071]] 

Predict small proportions:
        real            pred
[[ 12.07904766  10.27740002]
 [ 12.07904766  10.28021336]
 [ 12.07904766  10.29475117]
 [ 12.07904766  10.25755119]
 [ 12.07904766  10.21598053]
 [ 12.07904766  10.22934055]
 [ 12.07904766  10.28066349]
 [ 12.48451277  10.24413586]
 [ 12.48451277  10.27627373]
 [ 12.07904766  10.24505806]
 [ 12.07904766  10.30801773]
 [ 12.48451277  10.29165173]
 [ 12.07904766  10.26035595]
 [ 12.07904766  10.2427845 ]
 [ 12.48451277  10.28197098]
 [ 12.07904766  10.27839661]
 [ 12.07904766  10.28106499]
 [ 13.17765995  10.29710865]
 [ 12.07904766  10.29539394]
 [ 12.07904766  10.20482826]]
```


## One-way LSTM with receptors with small proportions, weighted

Model: same as the previous one, but every receptor has a weight equal to it's count in the experimental data.

After 673 iterations:
```
loss: 128.2314

Predict big proportions:
  real    pred
[[  5.0744682    8.53262615]
 [  5.22786273   8.54284096]
 [  5.50443683   8.53262615]
 [  6.25304755   9.09291649]
 [  6.33391      9.03355789]
 [  6.38307337   9.10030842]
 [  6.37860409   8.54321957]
 [  6.5170848    9.12787914]
 [  6.5273809    8.53403854]
 [  6.60817853   8.94172001]
 [  6.61521585   8.99175358]
 [  6.71463049   9.41586113]
 [  6.94914894   9.08821678]
 [  6.92190991   8.56362724]
 [  6.92383114   9.06714916]
 [  7.0442619    9.04962826]
 [  7.17130679   9.06808376]
 [  7.16884676   8.56467152]
 [  7.17130679   8.56088161]
 [  7.21923525  10.70879078]] 

Predict small proportions:
  real    pred
[[ 10.98043537   9.42418957]
 [ 11.09821841  10.05135345]
 [ 10.98043537   9.71676636]
 [ 10.98043537   9.09264469]
 [ 10.98043537   9.45188808]
 [ 10.98043537   9.01869011]
 [ 11.09821841   9.19659233]
 [ 11.09821841   9.20628262]
 [ 11.2317498    9.091609  ]
 [ 10.98043537   8.97654343]
 [ 10.98043537   9.16755676]
 [ 10.98043537   8.87772369]
 [ 10.98043537   9.67287254]
 [ 10.98043537   9.64953613]
 [ 10.98043537   8.8637476 ]
 [ 10.98043537   9.36828041]
 [ 11.2317498    8.96508598]
 [ 10.98043537   8.97455311]
 [ 11.09821841   9.0380621 ]
 [ 11.09821841  10.63829136]]
```


## One-way LSTM with 256 hidden nodes with receptors with small proportions, weighted

Model: same as the previous one, but with 256 hidden nodes.

After 673 iterations:
```
loss: 128.2314

Predict big proportions:
  real    pred
[[  5.0744682    8.34117031]
 [  5.22786273   8.45107937]
 [  5.50443683   8.34117031]
 [  6.25304755   9.04088879]
 [  6.33391      8.69348145]
 [  6.38307337   8.75631809]
 [  6.37860409   8.36138153]
 [  6.5170848   10.34063435]
 [  6.5273809    8.45126915]
 [  6.60817853   8.67682457]
 [  6.61521585   8.76920795]
 [  6.71463049   8.93307018]
 [  6.94914894   8.96351528]
 [  6.92190991   8.50222969]
 [  6.92383114   8.95307064]
 [  7.0442619    8.88143158]
 [  7.17130679   8.87015057]
 [  7.16884676   8.54760933]
 [  7.17130679   8.40906239]
 [  7.21923525   9.42609501]] 

Predict small proportions:
  real    pred
[[ 10.98043537   8.99558353]
 [ 11.09821841   9.45848179]
 [ 10.98043537   9.02121449]
 [ 10.98043537   8.95403004]
 [ 10.98043537   9.35338306]
 [ 10.98043537   9.16096878]
 [ 11.09821841   9.09424973]
 [ 11.09821841   8.97750092]
 [ 11.2317498    8.9752779 ]
 [ 10.98043537   9.30839157]
 [ 10.98043537   9.05585957]
 [ 10.98043537   9.04148865]
 [ 10.98043537   9.09450245]
 [ 10.98043537   9.25931358]
 [ 10.98043537   8.73466778]
 [ 10.98043537   9.50121307]
 [ 11.2317498    8.82824707]
 [ 10.98043537   8.93473339]
 [ 11.09821841   8.93610191]
 [ 11.09821841   9.20418262]]
```